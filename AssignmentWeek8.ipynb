{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Assignment  week 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Joep Straatman, Steven de Weille\n",
    "\n",
    "__Student id(s)__ : \n",
    "\n",
    "Link to presentation: https://docs.google.com/presentation/d/1Yuzv0-9O1rGUC9sf8T2msBE9VZ2TE5n8Xod-aJ-0zI0/edit?usp=sharing\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='link to your selfie'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment \n",
    "\n",
    "\n",
    "### Standard option\n",
    "Create a search engine for one of the collections listed below:\n",
    "\n",
    "\n",
    "#### Examples of collections\n",
    "\n",
    "* Wikipedia: dumps are available at <https://dumps.wikimedia.org/nlwiki/latest/> You should get the file <https://dumps.wikimedia.org/nlwiki/latest/nlwiki-latest-pages-articles.xml.bz2> (everything) or a smaller part to start with, e.g., <https://dumps.wikimedia.org/nlwiki/latest/nlwiki-latest-pages-articles1.xml.bz2>\n",
    "* Enron email dataset: see e.g. <https://www.cs.cmu.edu/~./enron/>\n",
    "* 400K questions and 1.4M answers from goeievraag.nl: <http://maartenmarx.nl/teaching/zoekmachines/Data/goeievraag.zip>\n",
    "* the _kamervragen_ collection also used before: <http://maartenmarx.nl/teaching/zoekmachines/Data/kvr.zip> (you know this dataset well, so make your assignment extra exciting)\n",
    "* ~~[Hillary Clinton Email collection](https://archive.org/details/hillary-clinton-emails-august-31-release) See also our <http://maartenmarx.nl/teaching/zoekmachines/Data/> folder.~~\n",
    "* [part of ArchivX](http://www.cs.cornell.edu/projects/kddcup/datasets.html) Scientific articles in latex, with lots of metadata\n",
    "* the <a href=\"http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html\">Reuters newspaper collection</a>  \n",
    "\n",
    "             \n",
    "            \n",
    "Here is an example of a (small) document from Reuters:            \n",
    "\n",
    "```\n",
    "<REUTERS TOPICS=\"NO\" LEWISSPLIT=\"TRAIN\" CGISPLIT=\"TRAINING-SET\" OLDID=\"5546\" NEWID=\"3\">\n",
    "<DATE>26-FEB-1987 15:03:27.51</DATE>\n",
    "<TOPICS></TOPICS>\n",
    "<PLACES><D>usa</D></PLACES>\n",
    "<PEOPLE></PEOPLE>\n",
    "<ORGS></ORGS>\n",
    "<EXCHANGES></EXCHANGES>\n",
    "<COMPANIES></COMPANIES>\n",
    "<UNKNOWN> \n",
    "&#5;&#5;&#5;F A\n",
    "&#22;&#22;&#1;f0714&#31;reute\n",
    "d f BC-TEXAS-COMMERCE-BANCSH   02-26 0064</UNKNOWN>\n",
    "<TEXT>&#2;\n",
    "<TITLE>TEXAS COMMERCE BANCSHARES &lt;TCB> FILES PLAN</TITLE>\n",
    "<DATELINE>    HOUSTON, Feb 26 - </DATELINE><BODY>Texas Commerce Bancshares Inc's Texas\n",
    "Commerce Bank-Houston said it filed an application with the\n",
    "Comptroller of the Currency in an effort to create the largest\n",
    "banking network in Harris County.\n",
    "    The bank said the network would link 31 banks having\n",
    "13.5 billion dlrs in assets and 7.5 billion dlrs in deposits.\n",
    "       \n",
    " Reuter\n",
    "&#3;</BODY></TEXT>\n",
    "</REUTERS>\n",
    "```\n",
    "   \n",
    "### Other options\n",
    "* Provided that you come up with a non trivial collection you can create a search engine using a different collection and also different software, as long as it is not done in MySQL.\n",
    "* Discuss this with the assistants.\n",
    "    * Convince us that your data set is interesting and your software solution worthwhile investigating.\n",
    "* For the presentations it is fun to see something else.\n",
    "* Usually it has a positive effect on your mark if you do something completely different ;-)\n",
    "* You should do the same requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Requirements</h2>\n",
    "        <p>Each of the following points <strong>must</strong> be addressed. Create a seperate page on the wiki for each point. Make sure these pages can be found from the menu of your wiki. \n",
    "        Explain what you did, and exemplify with links to screenshots/a working system.</p>\n",
    "        <ol>\n",
    "        <li>Search as we know it from Google. Give a result page (SERP), with links to the documents and some description of each hit.</li>\n",
    "            <li>Advanced search. Let a user be able to search in several fields, also in several fields simulteanously. Queries like \"return articles with a title  about XXX  and which are   about YYY in the period ZZZ\" should be possible.\n",
    "             </li>\n",
    "        <li>Do one of the following:\n",
    "            <ol><li>Represent the hits of a query with a wordcloud of 25-50 informative words. The wordcloud should somehow summarise what the collection has to say about the query.\n",
    "            You may think of these words as words that you could add to the query in order to improve recall (blind relevance feedback/query expansion). </li>\n",
    "                <li>Represent each document   with a word-cloud. </li></ol>\n",
    "        <br/>You can use several techniques to get rid of high frequency, but meaningless words: of course IDF, but also mutual information (see 13.5.1), or of course the technique from the paper by Kaptein et al on wordclouds.\n",
    "        </li>\n",
    "          <!--  <li>Group (nearly) identical documents. It is bad if two or more of the precious places of your top ten hits are occupied by near identical copies of a document.\n",
    "            Even more so if it is not relevant! So, get rid of these, by grouping them for instance. You get an idea of these duplicates if you search for \"neuken\".</li>\n",
    "            -->\n",
    "            <li>Give next to a traditional list of results, a timeline in which you indicate how many hits there are over time.</li>\n",
    "            <li>Provide Faceted Search next to the traditional list of results. For the \"Reuters\" collection, use the Category information as facet values.</li>\n",
    "            <li><strong>Evaluate your results</strong> Let 2 persons assess the relevancy of the top 10 documents for <strong>5 different queries</strong>. Compute Cohen's kappa. Determine the average precision at 10 for your system based on these 5 queries, and the two relevance assesments. \n",
    "                Also plot the P@10 (for both judges) for each query, showing differences in hard and easy queries.  \n",
    "                Describe clearly how you solved differences in judgements.\n",
    "            <br/>\n",
    "            Create your queries in the following format:\n",
    "<pre>\n",
    "&lt;topic number=\"6\"  >\n",
    "    &lt;query>kcs&lt;/query>\n",
    "    &lt;description>Find information on the Kansas City Southern railroad.\n",
    "    &lt;/description>\n",
    "\n",
    "&lt;/topic>\n",
    "\n",
    "&lt;topic number=\"16\"  >\n",
    "    &lt;query>arizona game and fish&lt;/query>\n",
    "    &lt;description>I'm looking for information about fishing and hunting\n",
    "    in Arizona.\n",
    "    &lt;/description>\n",
    "&lt;/topic>\n",
    "</pre>\n",
    "    So, both provide the actual query, and a description of the information need that was behind the query.\n",
    "    <br/>\n",
    "    Give a small set of clear guidelines for judging the results, and let your judges follow these guidelines.\n",
    "    <br/>\n",
    "    It is far more interesting to have difficult queries (both for the search engine and for the judges) than to have queries on which all ten retrieved documents are relevant.\n",
    "    So, try to create a good list of information needs.\n",
    "\n",
    "</li>\n",
    "<li>Change the ranking of your system, compute the average precision at 10 using your 5 queries, compare the results to your old system, and EXPLAIN what is going on.</li>\n",
    "</ol>\n",
    "\n",
    "<h2>Presentation</h2>\n",
    "<p>During your presentation you should have a live working search engine, that you demonstrate on the spot. Your presentation should be structured so that you will show all  requirements.\n",
    "You will be asked to show how your system works using information needs coming  from the audience.</p>\n",
    "<p>**Hint:** focus on a special aspect of your project. Everyone has done something similar, so your audience knows what was hard and what was terrible. Pick something you think will interest them.</p>\n",
    "\n",
    "\n",
    "<h2>How you will be marked</h2>\n",
    "<ul>\n",
    "    <li>Sent the URL of your guthub wiki to Maarten Marx BEFORE the presentation.</li>\n",
    "    <li>The first page of the wiki should contain:\n",
    "        <ol>\n",
    "            <li>The names and student numbers of the project members</li>\n",
    "            <li>A link to the slides of your presentation</li>\n",
    "            <li>A table of contents, with links to pages on the wiki adressing one of each of the \"must-have\" points listed above.\n",
    "            <br/>During grading, you will receive points for each of the points. So make it crystal clear where they are adressed in your wiki. Use one page per point.</li>\n",
    "\n",
    "\n",
    "</ol>\n",
    "</li>\n",
    "<li>The page for each point should contain, all rather briefly,\n",
    "<ol>\n",
    "<li>What you did and why you choose to do it in your special way.</li>\n",
    "<li>Examples of what works, and what does not work (very well).</li>\n",
    "<li>An evaluation of the quality of your work in 3-4 sentences.</li>\n",
    "</ol></li>\n",
    "<li>Clickable links to a live working demo are <strong>highly appreciated</strong>. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reut2-000.sgm\n",
      "reut2-001.sgm\n",
      "reut2-002.sgm\n",
      "reut2-003.sgm\n",
      "reut2-004.sgm\n",
      "reut2-005.sgm\n",
      "reut2-006.sgm\n",
      "reut2-007.sgm\n",
      "reut2-008.sgm\n",
      "reut2-009.sgm\n",
      "reut2-010.sgm\n",
      "reut2-011.sgm\n",
      "reut2-012.sgm\n",
      "reut2-013.sgm\n",
      "reut2-014.sgm\n",
      "reut2-015.sgm\n",
      "reut2-016.sgm\n",
      "reut2-017.sgm\n",
      "reut2-018.sgm\n",
      "reut2-019.sgm\n",
      "reut2-020.sgm\n",
      "reut2-021.sgm\n",
      "Done reading files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "l = []\n",
    "for f in glob.glob(\"*.sgm\"):\n",
    "    print f\n",
    "    with open(f) as file:\n",
    "        text= BeautifulSoup(file.read(), 'lxml')\n",
    "    l.append(text)\n",
    "print \"Done reading files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use only one file for programming and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onefile = l[0]\n",
    "onefile = onefile.findAll(\"reuters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<reuters cgisplit=\"TRAINING-SET\" lewissplit=\"TRAIN\" newid=\"3\" oldid=\"5546\" topics=\"NO\">\\n<date>26-FEB-1987 15:03:27.51</date>\\n<topics></topics>\\n<places><d>usa</d></places>\\n<people></people>\\n<orgs></orgs>\\n<exchanges></exchanges>\\n<companies></companies>\\n<unknown> \\nF A\\nf0714reute\\nd f BC-TEXAS-COMMERCE-BANCSH   02-26 0064</unknown>\\n<text>\\n<title>TEXAS COMMERCE BANCSHARES &lt;TCB&gt; FILES PLAN</title>\\n<dateline>    HOUSTON, Feb 26 - </dateline>Texas Commerce Bancshares Inc's Texas\\nCommerce Bank-Houston said it filed an application with the\\nComptroller of the Currency in an effort to create the largest\\nbanking network in Harris County.\\n    The bank said the network would link 31 banks having\\n13.5 billion dlrs in assets and 7.5 billion dlrs in deposits.\\n       \\n Reuter\\n</text>\\n</reuters>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show how reuters document looks like\n",
    "onefile[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Converting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Rewrite date to yyyymmdd format\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt, mpld3\n",
    "import collections\n",
    "def rewrite_date(date):\n",
    "    new = date.split('-')\n",
    "    if len(new[0]) == 1:\n",
    "        day = '0' +new[0]\n",
    "    else:\n",
    "        day = new[0]\n",
    "    datum = {'JAN' : '01', 'FEB' : '02', 'MAR' : '03', 'APR' : '04', 'MAY': '05', 'JUN' : '06',\n",
    "             'JUL' : '07', 'AUG' : '08', 'SEP' : '09', 'OCT' :'10', 'NOV' :'11', 'DEC' : '12' }\n",
    "    ndat = datum[new[1]]\n",
    "    \n",
    "    return str(new[2] + ndat + day)\n",
    "\n",
    "def plotit(text):\n",
    "    token = nltk.word_tokenize(text)\n",
    "   \n",
    "    nonstoplist = [word for word in token if word not in stopwords.words('english')]\n",
    "    \n",
    "    counter = collections.Counter(nonstoplist)\n",
    "    wcp = (counter.most_common(5))\n",
    "    ##nonstoplist = ''.join(word for word in str(nonstoplist))\n",
    "    #print nonstoplist\n",
    "    \n",
    "    #wordcloud = WordCloud(    background_color='white',\n",
    "    #                              width=1200,\n",
    "    #                             height=1000\n",
    "    #                             ).generate(nonstoplist)\n",
    "\n",
    "    ##wordcloud = WordCloud(max_font_size=40).generate(nonstoplist)\n",
    "    #plt.figure()\n",
    "    ##plt.imshow(wordcloud,origin='lower')\n",
    "    ##plt.axis(\"off\")\n",
    "    \n",
    "    #plt.plot()\n",
    "    #plt.imshow(wordcloud)\n",
    "    #plotje = plt.axis('off').plot\n",
    "    ##mpld3.show()\n",
    "    #wcp = mpld3.fig_to_html(wcp)\n",
    "    #wcp = mpld3.fig_to_dict()\n",
    "    \n",
    "    \n",
    "    return wcp\n",
    "\n",
    "# Convert Reuters document to JSON format\n",
    "def parse_doc(doc):\n",
    "    \n",
    "    soup = BeautifulSoup(doc)\n",
    "    # id\n",
    "    nid = soup.reuters['newid']\n",
    "    # date\n",
    "    date = soup.date.text[:11].strip()\n",
    "    date = rewrite_date(date)\n",
    "    # topics\n",
    "    topics = []\n",
    "    topic = soup.topics.find_all('d')\n",
    "    for top in topic:\n",
    "        topics.append(top.get_text())\n",
    "    # places\n",
    "    places = []\n",
    "    place = soup.places.find_all('d')\n",
    "    for pla in place:\n",
    "        places.append(pla.get_text())\n",
    "    # people\n",
    "    people = []\n",
    "    ppl = soup.people.find_all('d')\n",
    "    for pp in ppl:\n",
    "        people.append(pp.get_text())\n",
    "    # orgs\n",
    "    orgs = []\n",
    "    orga = soup.orgs.find_all('d')\n",
    "    for org in orga:\n",
    "        orgs.append(org.get_text())\n",
    "    # exchanges\n",
    "    exchanges = []\n",
    "    exchange = soup.exchanges.find_all('d')\n",
    "    for exc in exchange:\n",
    "        exchanges.append(exc.get_text())\n",
    "    # copanies\n",
    "    companies = []\n",
    "    company = soup.companies.find_all('d')\n",
    "    for comp in company:\n",
    "        companies.append(com.get_text())\n",
    "    # title\n",
    "    try:\n",
    "        title = soup.title.text.lower()\n",
    "    except:\n",
    "        title = ''\n",
    "    # text\n",
    "    try:\n",
    "        text = soup.find('dateline').next_sibling\n",
    "    except:\n",
    "        text = ''\n",
    "    # JSON format for ES\n",
    "    wc = plotit(text)\n",
    "    doc = {\n",
    "        'id': nid,\n",
    "        'date': date,\n",
    "        'topics': topics,\n",
    "        'places': places,\n",
    "        'people': people,\n",
    "        'orgs': orgs,\n",
    "        'exchanges': exchanges,\n",
    "        'companies': companies,\n",
    "        'title': title,\n",
    "        'text': text,\n",
    "        #'wc' : wc,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return doc\n",
    "\n",
    "# convert multiple documents to list of json dicts\n",
    "def convert_file(documents):\n",
    "    result = []\n",
    "    for doc in documents:\n",
    "        document = parse_doc(str(doc))\n",
    "        result.append(document)\n",
    "    return result\n",
    "\n",
    "# convert one sgml file for programming/testing\n",
    "# TO-DO: for all files\n",
    "\n",
    "documents = convert_file(onefile)\n",
    "print len(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Reuters Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing 'reuters' index\n",
      "Created new (empty) 'reuters' index\n",
      "\n",
      "health status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size\n",
      "yellow open   megacorp 95WfcsCYQwu_MGtgOzFDRw   5   1          1            0      6.4kb          6.4kb\n",
      "yellow open   reuters  r8TNBhAkTvu4jiP-5xQ9TA   5   1          0            0       810b           810b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   312  100   312    0     0  10064      0 --:--:-- --:--:-- --:--:-- 10064\n"
     ]
    }
   ],
   "source": [
    "# !!! Run elasticsearch first\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "HOST = 'http://localhost:9200/'\n",
    "es = Elasticsearch(hosts=[HOST])\n",
    "\n",
    "# (re)create reuters index\n",
    "es.indices.delete(index='reuters', ignore=[400,404])\n",
    "print \"Deleted existing 'reuters' index\"\n",
    "es.indices.create(index='reuters', ignore=400)\n",
    "print \"Created new (empty) 'reuters' index\\n\"\n",
    "\n",
    "# verify creation\n",
    "! curl \"localhost:9200/_cat/indices?v\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1000 documents to index\n",
      "health status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size\n",
      "yellow open   megacorp 95WfcsCYQwu_MGtgOzFDRw   5   1          1            0      6.4kb          6.4kb\n",
      "yellow open   reuters  r8TNBhAkTvu4jiP-5xQ9TA   5   1         97            0       810b           810b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   312  100   312    0     0  19500      0 --:--:-- --:--:-- --:--:-- 19500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'_shards': {u'failed': 0, u'skipped': 0, u'successful': 5, u'total': 5},\n",
       " u'count': 97}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Somehow you have to run this cell twice to add all documents\n",
    "\n",
    "# Fill index with list of documents\n",
    "def ingest_index(documents):\n",
    "    k = ({'_type':'document', '_index':'reuters', '_id':doc['id'],\n",
    "        '_source':doc} for doc in documents)\n",
    "    helpers.bulk(es,k)\n",
    "    print 'Added %d documents to index' % len(documents)\n",
    "\n",
    "ingest_index(documents)\n",
    "\n",
    "# Verify documents are ingested\n",
    "! curl \"localhost:9200/_cat/indices?v\"\n",
    "es.count(index='reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_shards': {u'failed': 0, u'skipped': 0, u'successful': 5, u'total': 5},\n",
       " u'hits': {u'hits': [{u'_id': u'3',\n",
       "    u'_index': u'reuters',\n",
       "    u'_score': 0.53351045,\n",
       "    u'_source': {u'companies': [],\n",
       "     u'date': u'19870226',\n",
       "     u'exchanges': [],\n",
       "     u'id': u'3',\n",
       "     u'orgs': [],\n",
       "     u'people': [],\n",
       "     u'places': [u'usa'],\n",
       "     u'text': u\"Texas Commerce Bancshares Inc's Texas\\nCommerce Bank-Houston said it filed an application with the\\nComptroller of the Currency in an effort to create the largest\\nbanking network in Harris County.\\n    The bank said the network would link 31 banks having\\n13.5 billion dlrs in assets and 7.5 billion dlrs in deposits.\\n       \\n Reuter\\n\",\n",
       "     u'title': u'texas commerce bancshares <tcb> files plan',\n",
       "     u'topics': []},\n",
       "    u'_type': u'document'},\n",
       "   {u'_id': u'11',\n",
       "    u'_index': u'reuters',\n",
       "    u'_score': 0.53351045,\n",
       "    u'_source': {u'companies': [],\n",
       "     u'date': u'19870226',\n",
       "     u'exchanges': [],\n",
       "     u'id': u'11',\n",
       "     u'orgs': [],\n",
       "     u'people': [],\n",
       "     u'places': [u'usa'],\n",
       "     u'text': u'Shr 34 cts vs 1.19 dlrs\\n    Net 807,000 vs 2,858,000\\n    Assets 510.2 mln vs 479.7 mln\\n    Deposits 472.3 mln vs 440.3 mln\\n    Loans 299.2 mln vs 327.2 mln\\n    Note: 4th qtr not available. Year includes 1985\\nextraordinary gain from tax carry forward of 132,000 dlrs, or\\nfive cts per shr.\\n Reuter\\n',\n",
       "     u'title': u'cobanco inc <cbco> year net',\n",
       "     u'topics': [u'earn']},\n",
       "    u'_type': u'document'},\n",
       "   {u'_id': u'17',\n",
       "    u'_index': u'reuters',\n",
       "    u'_score': 0.53351045,\n",
       "    u'_source': {u'companies': [],\n",
       "     u'date': u'19870226',\n",
       "     u'exchanges': [],\n",
       "     u'id': u'17',\n",
       "     u'orgs': [],\n",
       "     u'people': [],\n",
       "     u'places': [u'usa'],\n",
       "     u'text': u'National Health Enhancement\\nSystems Inc said it is offering a new health evaluation system\\nto its line of fitness assessment programs.\\n    The company said the program, called The Health Test, will\\nbe available in 60 days.\\n    Customers who use the program will receive a\\ncomputer-generated report and recommendations for implementing\\na program to improve their physical condition.\\n Reuter\\n',\n",
       "     u'title': u'national health enhancement <nhes> new program',\n",
       "     u'topics': []},\n",
       "    u'_type': u'document'},\n",
       "   {u'_id': u'23',\n",
       "    u'_index': u'reuters',\n",
       "    u'_score': 0.53351045,\n",
       "    u'_source': {u'companies': [],\n",
       "     u'date': u'19870226',\n",
       "     u'exchanges': [],\n",
       "     u'id': u'23',\n",
       "     u'orgs': [],\n",
       "     u'people': [],\n",
       "     u'places': [u'usa'],\n",
       "     u'text': u'Brown-Forman Inc said its board\\nhas approved a three-for-two stock split and a 35 pct increase\\nin the company cash dividend.\\n    The company cited its improved earnings outlook and\\ncontinued strong cash flow as reasons for raising the dividend.\\n    Brown-Forman said the split of its Class A and Class B\\ncommon shares would be effective March 13.\\n    The company said directors declared a quarterly cash\\ndividend on each new share of both classes of 28 cts, payable\\nApril one to holders of record March 20. Prior to the split,\\nthe company had paid 31 cts quarterly.\\n    Brown-Forman today reported a 37 pct increase in third \\nquarter profits to 21.6 mln dlrs, or 1.00 dlr a share, on a\\nseven pct increase in sales to a record 337 mln dlrs.\\n    Brown-Forman said nine month profits declined a bit to 66.0\\nmln dlrs, or 3.07 dlrs a share, from 66.2 mln dlrs, or 3.08\\ndlrs a share, a year earlier due to a second quarter charge of\\n37 cts a share for restructuring its beverage operations.\\n    The company said lower corporate tax rates and the\\nrestructuring \"are expected to substantially improve\\nBrown-Forman\\'s earnings and cash flow in fiscal 1988.\"\\n Reuter\\n',\n",
       "     u'title': u'brown-forman <bfdb> sets stock split, ups payout',\n",
       "     u'topics': [u'earn']},\n",
       "    u'_type': u'document'},\n",
       "   {u'_id': u'31',\n",
       "    u'_index': u'reuters',\n",
       "    u'_score': 0.53351045,\n",
       "    u'_source': {u'companies': [],\n",
       "     u'date': u'19870226',\n",
       "     u'exchanges': [],\n",
       "     u'id': u'31',\n",
       "     u'orgs': [],\n",
       "     u'people': [],\n",
       "     u'places': [u'usa'],\n",
       "     u'text': u'',\n",
       "     u'title': u'u.s. tax writers seek estate tax curbs, raising 6.7 billion dlrs thru 1991\\n',\n",
       "     u'topics': []},\n",
       "    u'_type': u'document'},\n",
       "   {u'_id': u'37',\n",
       "    u'_index': u'reuters',\n",
       "    u'_score': 0.53351045,\n",
       "    u'_source': {u'companies': [],\n",
       "     u'date': u'19870226',\n",
       "     u'exchanges': [],\n",
       "     u'id': u'37',\n",
       "     u'orgs': [],\n",
       "     u'people': [],\n",
       "     u'places': [u'usa'],\n",
       "     u'text': u'Shr loss 22 cts vs loss 18 cts\\n    Net loss 3,035,000 vs loss 2,516,000\\n    Revs 20.9 mln vs 19.6 mln\\n    Qtly div three cts vs three cts prior\\n    Year\\n    Shr profit two cts vs profit 34 cts\\n    Net profit 215,000 vs profit 4,647,000\\n    Revs 93.4 mln vs 98.7 mln\\n    NOTE: Dividend payable April one to shareholders of record\\nMarch 17.\\n Reuter\\n',\n",
       "     u'title': u'computer language research in <clri> 4th qtr',\n",
       "     u'topics': [u'earn']},\n",
       "    u'_type': u'document'},\n",
       "   {u'_id': u'45',\n",
       "    u'_index': u'reuters',\n",
       "    u'_score': 0.53351045,\n",
       "    u'_source': {u'companies': [],\n",
       "     u'date': u'19870226',\n",
       "     u'exchanges': [],\n",
       "     u'id': u'45',\n",
       "     u'orgs': [],\n",
       "     u'people': [],\n",
       "     u'places': [u'usa'],\n",
       "     u'text': u'ChemLawn Corp <CHEM> could attract a\\nhigher bid than the 27 dlrs per share offered by Waste\\nManagement Inc <WNX>, Wall Street arbitrageurs said.\\n    Shares of ChemLawn shot up 11-5/8 to 29-3/8 in\\nover-the-counter- trading with 3.8 mln of the company\\'s 10.1\\nmln shares changing hands by late afternoon.\\n    \"This company could go for 10 times cash flow or 30 dlrs,\\nmaybe 32 dollars depending on whether there is a competing\\nbidder,\" an arbitrageur said. Waste Management\\'s tender offer,\\nannounced before the opening today, expires March 25.\\n    \"This is totally by surprise,\" said Debra Strohmaier, a\\nChemLawn spokeswoman. The company\\'s board held a regularly\\nscheduled meeting today and was discussing the Waste Management\\nannouncement. She said a statement was expected but it was not\\ncertain when it would be ready. \\n    She was unable to say if there had been any prior contact\\nbetween Waste Management and ChemLawn officials.\\n    \"I think they will resist it,\" said Elliott Schlang,\\nanalyst at Prescott, Ball and Turben Inc. \"Any company that\\ndoesn\\'t like a surprise attack would.\"\\n    Arbitrageurs pointed out it is difficult to resist tender\\noffers for any and all shares for cash. Schlang said ChemLawn\\ncould try to find a white knight if does not want to be\\nacquired by Waste Management.\\n    Analyst Rosemarie Morbelli of Ingalls and Snyder said\\nServiceMaster Companies L.P. <SVM> or Rollins Inc <ROL> were\\nexamples of companies that could be interested.\\n    ChemLawn, with about two mln customers, is the largest U.S.\\ncompany involved in application of fertilizers, pesticides and\\nherbicides on lawns. Waste Management is involved in removal of\\nwastes.\\n    Schlang said ChemLawn\\'s customer base could be valuable to\\nanother company that wants to capitalize on a strong\\nresidential and commercial distribution system.\\n    Both Schlang and Morbelli noted that high growth rates had\\ncatapulted ChemLawn\\'s share price into the mid-30\\'s in 1983 but\\nthe stock languished as the rate of growth slowed.\\n    Schlang said the company\\'s profits are concentrated in the\\nfourth quarter. In 1986 ChemLawn earned 1.19 dlrs per share for\\nthe full year, and 2.58 dlrs in the fourth quarter.\\n    Morbelli noted ChemLawn competes with thousands of\\nindividual entrepreuers who offer lawn and garden care sevice.\\n Reuter\\n',\n",
       "     u'title': u'chemlawn <chem> rises on hopes for higher bids',\n",
       "     u'topics': [u'acq']},\n",
       "    u'_type': u'document'},\n",
       "   {u'_id': u'50',\n",
       "    u'_index': u'reuters',\n",
       "    u'_score': 0.53351045,\n",
       "    u'_source': {u'companies': [],\n",
       "     u'date': u'19870226',\n",
       "     u'exchanges': [],\n",
       "     u'id': u'50',\n",
       "     u'orgs': [],\n",
       "     u'people': [],\n",
       "     u'places': [u'usa'],\n",
       "     u'text': u'<America First Federally Guaranteed\\nMortgage Fund Two> said it is making a special distribution of\\n71.6 cts per exchangeable unit, which includes 67.62 cts from\\nreturn on capital and 3.98 cts from income gains.\\n Reuter\\n',\n",
       "     u'title': u'america first mortgage sets special payout',\n",
       "     u'topics': [u'earn']},\n",
       "    u'_type': u'document'},\n",
       "   {u'_id': u'53',\n",
       "    u'_index': u'reuters',\n",
       "    u'_score': 0.53351045,\n",
       "    u'_source': {u'companies': [],\n",
       "     u'date': u'19870226',\n",
       "     u'exchanges': [],\n",
       "     u'id': u'53',\n",
       "     u'orgs': [],\n",
       "     u'people': [],\n",
       "     u'places': [u'usa'],\n",
       "     u'text': u'Qtly div 35 cts vs 35 cts prior\\n    Payable March 31\\n    Record March nine\\n\\n Reuter\\n',\n",
       "     u'title': u'emhart corp <emh> qtly dividend',\n",
       "     u'topics': [u'earn']},\n",
       "    u'_type': u'document'},\n",
       "   {u'_id': u'56',\n",
       "    u'_index': u'reuters',\n",
       "    u'_score': 0.53351045,\n",
       "    u'_source': {u'companies': [],\n",
       "     u'date': u'19870226',\n",
       "     u'exchanges': [],\n",
       "     u'id': u'56',\n",
       "     u'orgs': [],\n",
       "     u'people': [],\n",
       "     u'places': [u'usa'],\n",
       "     u'text': u'AM International Inc, reporting an\\noperating loss for the January 31 second quarter, said\\nprospects for the balance of the fiscal year remain good.\\n    It said orders at its Harris Graphics subsidiary, acquired\\nin June 1986, \"continue to run at a strong pace.\" For the six\\nmonths, orders rose 35 pct over the corresponding prior-year\\nperiod, or on an annualized basis are running at about 630 mln\\ndlrs.\\n    The backlog at Harris is up 30 pct from the beginning of\\nthe fiscal year, AM said.\\n    AM International said its old division are expected to\\nbenefit from recent new product introductions and the decline\\nin the value of the dollar.\\n    \"Research, development and engineering expenditures in\\nfiscal 1987 will be in the 45-50 mln dlr range, and the company\\nsaid it has allocated another 30-40 mln dlrs for capital\\nexpenditures.\\n    Earlier AM reported a fourth quarter operating loss of two\\ncts a share compared to profits of seven cts a share a year\\nago. Revenues rose to 291.8 mln dlrs from 151.1 mln dlrs.\\n Reuter\\n',\n",
       "     u'title': u'am international <am> cites strong prospects',\n",
       "     u'topics': [u'earn']},\n",
       "    u'_type': u'document'}],\n",
       "  u'max_score': 0.53351045,\n",
       "  u'total': 546},\n",
       " u'timed_out': False,\n",
       " u'took': 3}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test query: find all documents with usa in places\n",
    "es.indices.refresh(index=\"reuters\")\n",
    "q= {\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"places\": \"usa\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "res = es.search(index=\"reuters\", body=q)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
